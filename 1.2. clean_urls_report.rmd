---
title: "Report: extraction of urls for IMF programs"
author: "Manuel BÃ©tin"
date: "21/11/2019"
output:
  html_document:
    number_sections: TRUE
    theme: sandstone
    highlight: tango
    code_folding: "hide"
abstract: "Overview of the extraction of urls using all_links.csv. The following code clean the urls by finding the country, the type of document and the type of program from the metadata. \n"

---


```{r setup,include=F}

#includes:
#      after_body: 5.rmarkdown/footer.html
##install common packages
library("devtools") #make sure you have the library
github_token=rio::import("/Users/manubetin/Dropbox/Manuel/Professionnel/github_token/github_token.txt")

#install_github("manuelbetin/SetUpProject",auth_token=github_token[[1]])
#install_github("manuelbetin/TextMiningCrisis",auth_token=github_token[[1]])

packages <- c("dplyr"
              , 'tictoc'
              , "rio"
              , "tidytext"
              , "stringr"
              , "stringi"
              , "tidyr"
              , "ggplot2"
              , "lubridate"
              , 'crayon'
              , "DT"
              , "plotly"
              , "TextMiningCrisis"
              , "SetUpProject")

## load common packages
SetUpProject::load.my.packages(packages)

```

```{r import data,warning=F}

links_all="urls_imf_consultations.RData"#
name_output=str_remove(links_all,"urls_imf_")
name_output=str_remove(name_output,".RData")
links_all=rio::import(paste0("2.data/urls docs/",links_all))

links_all$type_doc_consultations %>% unique()

if(str_detect(name_output,"consultations")){
url_links=links_all %>% 
  #filter(is.na(type_doc_programs)) %>% 
  filter(type_doc_consultations %in% c("Article IV","Eco developments","consultations") | !is.na(type_doc_programs)) %>%
  group_by(iso3_from_title,period,year,type_doc_consultations) %>% 
  summarize_all(funs(first)) %>% 
  ungroup() %>%
  mutate(name_file=paste0(iso3_from_title,"_",period,"_",type_doc_programs))

}else{
  url_links=links_all
  }
#a=url_links[1:1000,]
IMF_programs=rio::import("2.data/programs/IMF_programs_dates.RData")

```

```{r clean urls,warning=F}

urls_clean=url_links #%>% filter(type_hierarchy %in% c("Clean")) #select links corresponding to EBS with no correction or supplements

datatable(urls_clean %>% summarise(n=n()))

```


```{r}
datatable(urls_clean %>% group_by(type_doc_consultations) %>% summarise(n=n()))
```


# Number of documents by country

```{r}
datatable(urls_clean %>% group_by(iso3_from_title) %>% summarise(n=n()))

```


# Merge datasets of requests with new documents

```{r merge request and urls}

df1 <- urls_clean
df2 <- IMF_programs
# merge tables to match requests with docum


df2 <- df2 %>%
  rename(startdate = Period) %>%
  rename(enddate = Date_Expiration) %>%
  rename(ID = ISO3_Code) 

df2=df2 %>%
  mutate(startdate_ext=startdate %m-% months(24),
         enddate_ext=enddate %m+% months(24))

df1 <- df1 %>%
  rename(Loss_Date = period) %>%
  #rename(perf_crit = Performance_criteria) %>%
  rename(perf_criteria = performance_criteria) %>%
  rename(ID = iso3_from_title)

# Joining data

dt_merged <- full_join(df1, df2, by="ID") %>% 
  filter(Loss_Date >= startdate_ext & Loss_Date <= enddate_ext)
#Keeping those who don't match

urls_clean=dt_merged
urls_clean=urls_clean %>% group_by(ID,Loss_Date,type_doc_consultations,type_doc_programs) %>% summarize_all(first)

```

```{r descriptive stats of urls}

links_by_year=url_links %>% group_by(year) %>% summarize(n=n())

clean_links_by_year=urls_clean %>% group_by(year) %>% summarize(n=n())

ggplotly(ggplot()+
  geom_bar(data=links_by_year,stat="identity",aes(x=year,y=n,fill="raw"))+
  #geom_bar(data=clean_links_by_year,stat="identity",aes(x=year,y=n,fill="clean"))+
  labs(title="All urls")+
  theme_bw()
)
```

## Requests

### Requests by type of programs

```{r Describe requests SBA}

links_by_year=urls_clean %>% filter(type_program%in%c("SBA")) %>% group_by(type_program,year) %>% summarize(n=n())

ggplotly(ggplot()+
 # geom_bar(data=programs_by_year,stat="identity",aes(x=year,y=n,fill="target"),color="black",alpha=0.5)+
  geom_bar(data=links_by_year,stat="identity",aes(x=year,y=n),color="black")+
  # geom_point(data=programs_by_year,stat="identity",aes(x=year,y=n),color="black",size=2)+
  labs(title="Requests for SBA")+
  #lims(y=c(0,45))+
  theme_bw())

```


```{r Describe requests EFF}

urls_clean$type_program %>% unique()

links_by_year=urls_clean %>% filter(type_program%in%c("EA","EFF")) %>% group_by(type_program,year) %>% summarize(n=n())

ggplotly(ggplot()+
 # geom_bar(data=programs_by_year,stat="identity",aes(x=year,y=n,fill="target"),color="black",alpha=0.5)+
  geom_bar(data=links_by_year,stat="identity",aes(x=year,y=n),color="black")+
  # geom_point(data=programs_by_year,stat="identity",aes(x=year,y=n),color="black",size=2)+
  labs(title="Requests for EA")+
  #lims(y=c(0,45))+
  theme_bw())

```


```{r export}
rio::export(df1,paste0("2.data/urls docs/Final_urls_",name_output,"_from_clean_report.RData"))
```

